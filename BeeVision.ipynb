{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"YKH2x2PipX8Y","executionInfo":{"status":"ok","timestamp":1692879610056,"user_tz":-180,"elapsed":3635,"user":{"displayName":"Ömer Özer","userId":"04440507596371271697"}}},"outputs":[],"source":["# Creating a dataset class for coco format\n","import os\n","import torch\n","import torch.utils.data\n","import torchvision\n","from PIL import Image\n","from pycocotools.coco import COCO\n","\n","class COCO23Dataset(torch.utils.data.Dataset):\n","    def __init__(self, root, annotation, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def __getitem__(self, index):\n","        coco = self.coco\n","        img_id = self.ids[index]\n","        ann_ids = coco.getAnnIds(imgIds=img_id)\n","        coco_annotation = coco.loadAnns(ann_ids)\n","        path = coco.loadImgs(img_id)[0]['file_name']\n","        img = Image.open(os.path.join(self.root, path))\n","\n","        num_objs = len(coco_annotation)\n","\n","        # Bounding boxes and their labels\n","        boxes = []\n","        labels = []\n","        for annotation in coco_annotation:\n","            xmin = annotation['bbox'][0]\n","            ymin = annotation['bbox'][1]\n","            xmax = xmin + annotation['bbox'][2]\n","            ymax = ymin + annotation['bbox'][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","            # Assign category_id as label\n","            labels.append(annotation['category_id'])\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        img_id = torch.tensor([img_id])\n","\n","        # Area and iscrowd fields if needed\n","        areas = [annotation['area'] for annotation in coco_annotation]\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        # Construct the annotations\n","        annotations = {\n","            \"boxes\": boxes,\n","            \"labels\": labels,\n","            \"image_id\": img_id,\n","            \"area\": areas,\n","            \"iscrowd\": iscrowd\n","        }\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, annotations\n","\n","    def __len__(self):\n","        return len(self.ids)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"jTsoFSO6pv6S","executionInfo":{"status":"ok","timestamp":1692879613469,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ömer Özer","userId":"04440507596371271697"}}},"outputs":[],"source":["# In my case, just added ToTensor as a transform\n","def get_transform():\n","    custom_transforms = []\n","    custom_transforms.append(torchvision.transforms.ToTensor())\n","    return torchvision.transforms.Compose(custom_transforms)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1692879615461,"user":{"displayName":"Ömer Özer","userId":"04440507596371271697"},"user_tz":-180},"id":"paJ4GIJEp_JK","outputId":"439cff9f-e2d2-4e4b-8f91-0302c4bfd7f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["# Select GPU if available with cuda else cpu\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24735,"status":"ok","timestamp":1692879641923,"user":{"displayName":"Ömer Özer","userId":"04440507596371271697"},"user_tz":-180},"id":"Gw2U8-j1rrgo","outputId":"c16312c8-cafe-4de2-cc7f-19f419960d6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# This line gives the python code access to the files on the drive. path:\"/content/drive/MyDrive/BeeVisionCase_OmerOzer/\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":922,"status":"ok","timestamp":1692879646402,"user":{"displayName":"Ömer Özer","userId":"04440507596371271697"},"user_tz":-180},"id":"Sp8JhjAZqG0O","outputId":"af3fb9c7-98df-4c15-8b9d-ff17fe699e07"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.60s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.63s)\n","creating index...\n","index created!\n"]}],"source":["from traitlets.traitlets import validate\n","# path to train data and coco file\n","# this path is coming from above line which give access to the drive folder. you can set your own path\n","train_data_dir = '/content/drive/MyDrive/BeeVisionCase_OmerOzer/training_random_images'\n","train_coco = '/content/drive/MyDrive/BeeVisionCase_OmerOzer/random_train_626_coco.json'\n","\n","# path to validation data and coco file\n","validate_data_dir = '/content/drive/MyDrive/BeeVisionCase_OmerOzer/validation_random_images'\n","validate_coco = '/content/drive/MyDrive/BeeVisionCase_OmerOzer/validation_random_coco.json'\n","\n","# Train  Dataset\n","my_dataset = COCO23Dataset(root=train_data_dir,\n","                          annotation=train_coco,\n","                          transforms=get_transform()\n","                          )\n","# Validate dataset\n","validate_dataset = COCO23Dataset(root=validate_data_dir,\n","                          annotation=validate_coco,\n","                          transforms=get_transform()\n","                          )\n","# collate_fn needs for batch\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","# Train Batch size\n","train_batch_size = 5\n","\n","# Validate Batch size\n","validate_batch_size = 5\n","\n","# Train DataLoader\n","data_loader = torch.utils.data.DataLoader(my_dataset,\n","                                          batch_size=train_batch_size,\n","                                          shuffle=True,\n","                                          collate_fn=collate_fn)\n","# validation dataloader\n","val_data_loader = torch.utils.data.DataLoader(validate_dataset,\n","                                          batch_size=validate_batch_size,\n","                                          shuffle=True,\n","                                          collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"XLx0MqxzsPzV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692879674256,"user_tz":-180,"elapsed":21917,"user":{"displayName":"Ömer Özer","userId":"04440507596371271697"}},"outputId":"71b8efd1-46d1-4954-fee4-62db02fb6e8b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 119MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=24, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=96, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":6}],"source":["#CREATE THE MODEL\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    return model\n","\n","\n","# 24 classes including background\n","num_classes = 24\n","model = get_model_instance_segmentation(num_classes)\n","\n","#if you want to start from pretrained model you can load the parameters here (this line loads my trained model and it does not need training)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/BeeVisionCase_OmerOzer/model_weights_14epoch_626_0172.pth'))\n","\n","# move model to the right device\n","model.to(device)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hOpZ9DIOZPNw","outputId":"d0890c0d-eed4-4346-d1d9-a2bfc6bb6751"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Avg Training loss in epoch: 0.000921867610443206\n","Epoch 1, Avg Training loss in epoch: 0.0016772403485245174\n","Epoch 1, Avg Training loss in epoch: 0.002448730348121552\n","Epoch 1, Avg Training loss in epoch: 0.003959435851327957\n","Epoch 1, Avg Training loss in epoch: 0.004802066477991286\n","Epoch 1, Avg Training loss in epoch: 0.0056231215832725405\n","Epoch 1, Avg Training loss in epoch: 0.0068242209298270086\n","Epoch 1, Avg Training loss in epoch: 0.007558973535658821\n","Epoch 1, Avg Training loss in epoch: 0.008366331931144472\n","Epoch 1, Avg Training loss in epoch: 0.009065949905013282\n","Epoch 1, Avg Training loss in epoch: 0.009864818659566697\n","Epoch 1, Avg Training loss in epoch: 0.010399899371559657\n","Epoch 1, Avg Training loss in epoch: 0.011140652710483187\n","Epoch 1, Avg Training loss in epoch: 0.012286361247774154\n","Epoch 1, Avg Training loss in epoch: 0.01317708371650605\n","Epoch 1, Avg Training loss in epoch: 0.014192750411374229\n","Epoch 1, Avg Training loss in epoch: 0.015101232048537996\n","Epoch 1, Avg Training loss in epoch: 0.015766718912692296\n","Epoch 1, Avg Training loss in epoch: 0.016670963061707362\n","Epoch 1, Avg Training loss in epoch: 0.017736342632108264\n","Epoch 1, Avg Training loss in epoch: 0.018557111067431315\n","Epoch 1, Avg Training loss in epoch: 0.019264913326691068\n","Epoch 1, Avg Training loss in epoch: 0.019989085457627735\n","Epoch 1, Avg Training loss in epoch: 0.02106121288878577\n","Epoch 1, Avg Training loss in epoch: 0.021728188922953983\n","Epoch 1, Avg Training loss in epoch: 0.02240652520032156\n","Epoch 1, Avg Training loss in epoch: 0.023423794834386735\n","Epoch 1, Avg Training loss in epoch: 0.024147255139218435\n","Epoch 1, Avg Training loss in epoch: 0.02545565017868602\n","Epoch 1, Avg Training loss in epoch: 0.02610371255921939\n","Epoch 1, Avg Training loss in epoch: 0.02734139555739978\n","Epoch 1, Avg Training loss in epoch: 0.02812221238300914\n","Epoch 1, Avg Training loss in epoch: 0.028929016242424648\n","Epoch 1, Avg Training loss in epoch: 0.02987703899779017\n","Epoch 1, Avg Training loss in epoch: 0.030898529444895094\n","Epoch 1, Avg Training loss in epoch: 0.03156976490503266\n","Epoch 1, Avg Training loss in epoch: 0.03213120837296758\n","Epoch 1, Avg Training loss in epoch: 0.03348660486794654\n","Epoch 1, Avg Training loss in epoch: 0.03419060798154937\n","Epoch 1, Avg Training loss in epoch: 0.03496527742771875\n","Epoch 1, Avg Training loss in epoch: 0.03591006728155272\n"]}],"source":["# Train the model with desired parameters\n","# parameters\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","\n","len_dataloader = len(data_loader)\n","num_epochs = 1\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    i = 0\n","    total_train_loss = 0\n","    for imgs, annotations in data_loader:\n","        i += 1\n","        imgs = list(img.to(device) for img in imgs)\n","        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","        loss_dict = model(imgs, annotations)\n","        losses = sum(loss for loss in loss_dict.values())\n","        total_train_loss += losses.item()\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        avg_train_loss = total_train_loss / len(data_loader)\n","        print(f\"Epoch {epoch+1}, Avg Training loss in epoch: {avg_train_loss}\")\n","    # Validation phase\n","    model.train()\n","    total_val_loss = 0\n","    with torch.no_grad():\n","        for imgs, annotations in val_data_loader:\n","            imgs = list(img.to(device) for img in imgs)\n","            annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","            loss_dict = model(imgs, annotations)\n","            losses = sum(loss for loss in loss_dict.values())\n","            total_val_loss += losses.item()\n","\n","    avg_val_loss = total_val_loss / len(val_data_loader)\n","    print(f\"Epoch {epoch+1}, Validation loss: {avg_val_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3p_pIz1YbB4"},"outputs":[],"source":["# If you want to save the parameters after training\n","torch.save(model.state_dict(), 'model_weights_14epoch_626_0172.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1692864055588,"user":{"displayName":"Ömer Özer","userId":"04440507596371271697"},"user_tz":-180},"id":"uBKnuRfTb9Mz","outputId":"2a048b1c-c8c7-4993-abad-e5ad068e74ad"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# Load the saved weights to test manually\n","model.load_state_dict(torch.load('model_weights_14epoch_626_0172.pth'))\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70317,"status":"ok","timestamp":1692879756687,"user":{"displayName":"Ömer Özer","userId":"04440507596371271697"},"user_tz":-180},"id":"CZSlpnQNRWfv","outputId":"dff64ab3-49fe-4734-e91c-f7ba83c660eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7640\n"]}],"source":["#A simple accuracy metric by considering how many bounding boxes predicted by the model match with the\n","#ground truth boxes (true positives), given a certain Intersection over Union (IoU) threshold.\n","def calculate_accuracy(model, data_loader, device, iou_threshold=0.5):\n","    model.eval()\n","\n","    total_predictions = 0\n","    correct_predictions = 0\n","\n","    with torch.no_grad():\n","        for imgs, annotations in data_loader:\n","            imgs = list(img.to(device) for img in imgs)\n","            model_predictions = model(imgs)\n","\n","            for prediction, ground_truth in zip(model_predictions, annotations):\n","                pred_boxes = prediction['boxes'].cpu().numpy()\n","                gt_boxes = ground_truth['boxes'].cpu().numpy()\n","\n","                for pred_box in pred_boxes:\n","                    for gt_box in gt_boxes:\n","                        iou = compute_iou(pred_box, gt_box)\n","                        if iou > iou_threshold:\n","                            correct_predictions += 1\n","                            break  # Only count the predicted box once\n","                total_predictions += len(pred_boxes)\n","\n","    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n","    return accuracy\n","\n","def compute_iou(boxA, boxB):\n","    # Determine the (x, y)-coordinates of the intersection rectangle\n","    xA = max(boxA[0], boxB[0])\n","    yA = max(boxA[1], boxB[1])\n","    xB = min(boxA[2], boxB[2])\n","    yB = min(boxA[3], boxB[3])\n","\n","    # Compute the area of intersection rectangle\n","    inter_area = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n","\n","    # Compute the area of both the prediction and ground-truth rectangles\n","    boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n","    boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n","\n","    # Compute the intersection over union\n","    iou = inter_area / float(boxA_area + boxB_area - inter_area)\n","\n","    return iou\n","\n","# Now call the function\n","accuracy = calculate_accuracy(model, val_data_loader, device)\n","print(f\"Accuracy: {accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":875,"output_embedded_package_id":"1kkHqhsTqmcfJ_dq9E4X9WtZ0S9OkyPrm"},"executionInfo":{"elapsed":41203,"status":"ok","timestamp":1692879901088,"user":{"displayName":"Ömer Özer","userId":"04440507596371271697"},"user_tz":-180},"id":"DRrcl9ozwab6","outputId":"a5f5bd6b-e978-4d89-84a1-383c15b522a5"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Jpeg input, Jpeg output with bbox's\n","import torchvision.transforms as T\n","from PIL import Image, ImageDraw, ImageFont\n","from IPython.display import display\n","\n","categories = [\n","    { \"supercategory\": \"type\", \"id\": 1, \"name\": \"usps-normal\" },\n","    { \"supercategory\": \"type\", \"id\": 2, \"name\": \"usps-sure\" },\n","    { \"supercategory\": \"type\", \"id\": 3, \"name\": \"usps-first\" },\n","    { \"supercategory\": \"type\", \"id\": 4, \"name\": \"usps-prio\" },\n","    { \"supercategory\": \"type\", \"id\": 5, \"name\": \"yellow\" },\n","    { \"supercategory\": \"type\", \"id\": 6, \"name\": \"id-below\" },\n","    { \"supercategory\": \"type\", \"id\": 7, \"name\": \"lex32\" },\n","    { \"supercategory\": \"type\", \"id\": 8, \"name\": \"dhl\" },\n","    { \"supercategory\": \"type\", \"id\": 9, \"name\": \"ls-small\" },\n","    { \"supercategory\": \"type\", \"id\": 10, \"name\": \"ls\" },\n","    { \"supercategory\": \"type\", \"id\": 11, \"name\": \"fedex-g\" },\n","    { \"supercategory\": \"type\", \"id\": 12, \"name\": \"small\" },\n","    { \"supercategory\": \"type\", \"id\": 13, \"name\": \"ups-ground\" },\n","    { \"supercategory\": \"type\", \"id\": 14, \"name\": \"fragile-warning\" },\n","    { \"supercategory\": \"type\", \"id\": 15, \"name\": \"usps-boxes\" },\n","    { \"supercategory\": \"type\", \"id\": 16, \"name\": \"usps-four-qr\" },\n","    { \"supercategory\": \"type\", \"id\": 17, \"name\": \"fedex-s\" },\n","    { \"supercategory\": \"type\", \"id\": 18, \"name\": \"unknown-f\" },\n","    { \"supercategory\": \"type\", \"id\": 19, \"name\": \"usps-parcel\" },\n","    { \"supercategory\": \"type\", \"id\": 20, \"name\": \"usps-media\" },\n","    { \"supercategory\": \"type\", \"id\": 21, \"name\": \"fedex-h\" },\n","    { \"supercategory\": \"type\", \"id\": 22, \"name\": \"fedex-e\" },\n","    { \"supercategory\": \"type\", \"id\": 23, \"name\": \"usps-3rdparty\" }\n","]\n","\n","id_to_name = {category[\"id\"]: category[\"name\"] for category in categories}\n","\n","\n","\n","model.eval()\n","\n","def predict_image(image_path, model, device, detection_threshold):\n","    # Load Image\n","    image = Image.open(image_path).convert(\"RGB\")\n","    transform = T.Compose([T.ToTensor()])\n","    image = transform(image)\n","\n","    # Push to GPU & Add batch dimension\n","    image = image.to(device).unsqueeze(0)\n","\n","    # Get predictions\n","    with torch.no_grad():\n","        prediction = model(image)\n","\n","    # Filter out predictions below a certain threshold\n","    boxes = prediction[0]['boxes'][prediction[0]['scores'] > detection_threshold].cpu().numpy()\n","    scores = prediction[0]['scores'][prediction[0]['scores'] > detection_threshold].cpu().numpy()\n","    labels = prediction[0]['labels'][prediction[0]['scores'] > detection_threshold].cpu().numpy()\n","\n","    return boxes, scores, labels, image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n","\n","def draw_boxes(boxes, scores, labels, image_np, id_to_name, color='red'):\n","    image = Image.fromarray((image_np * 255).astype('uint8'))\n","    draw = ImageDraw.Draw(image)\n","    font = ImageFont.truetype(\"/content/drive/MyDrive/BeeVisionCase_OmerOzer/arial.ttf\", 50)\n","\n","    for box, score, label in zip(boxes, scores, labels):\n","        class_name = id_to_name[label]\n","        draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=color, width=6)\n","        draw.text((box[0] +25, box[1] - 55), text=f\"{class_name} {score:.2f}\", fill=color, font=font)\n","\n","    return image\n","\n","\n","\n","# Test with example images from validation dataset\n","#image_path = \"/content/drive/MyDrive/BeeVisionCase_OmerOzer/21577.jpeg\"\n","image_path = \"/content/drive/MyDrive/BeeVisionCase_OmerOzer/21568.jpeg\"\n","\n","boxes, scores, labels, img_np = predict_image(image_path, model, device, 0.4)\n","image_with_boxes = draw_boxes(boxes, scores, labels, img_np, id_to_name)\n","display(image_with_boxes)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOFXFwKyjZd1yB/LhfSDktn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}